---
layout: default
---


<div class="home">

    <h1>About</h1>
    {% include image.html url="/images/IMG_9402.jpg" caption="Aishwarya Kamath" width=300 align="center" %}
    <p> I am a PhD student at New York University's Center for Data Science, advised by Prof. Yann LeCun and Prof.
        Kyunghyun Cho. My research focuses on using information from multiple sources such as text, images, video and
        speech to improve commonsense reasoning capabilities of machines. Prior to this, I was advised during my Masters
        by Prof. Andrew McCallum at University of Massachusetts Amherst in areas of natural language processing and
        machine learning, with a special focus on structured prediction.</p>
    <!-- <p> I have also worked as a Machine Learning Engineer as part of the Machine Learning Research Group at Oracle Labs in Burlington, MA.  </p>
    <p> When I am not busy doing coursework or research, I love to paint and when time permits, keep my hobby of playing basketball alive. </p> -->
    <br>
    <p> If you are a NYU Master's student looking to work on research, especially related to multi-modal learning, feel
        free to reach out to me! :) </p>
    <hr>
    <br>

</div>


<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
    <tr>
        <td style="padding:20px;width:100%;vertical-align:middle">
            <h1>Research</h1>
        </td>
    </tr>
    </tbody>
</table>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
<td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/mdetr/pink.jpg' width="300">
              </div>
            </td>
<td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://ashkamath.github.io/mdetr_page">
        <font color="black"><strong>MDETR - Modulated Detection for End-to-End Multi-modal Understanding</strong></font>
    </a>
    <br>
    <strong><a href="https://www.semanticscholar.org/author/Aishwarya-Kamath/46174952">Aishwarya
    Kamath</a></strong>,
    <a href="https://scholar.google.com/citations?hl=en&user=QOO8OCcAAAAJ">Mannat Singh</a>,
    <a href="https://scholar.google.com/citations?hl=en&user=WLN3QrAAAAAJ">Yann LeCun</a>,
    <a href="https://scholar.google.com/citations?hl=en&user=wN9rBkcAAAAJ">Gabriel Synnaeve</a>,
    <a href="https://scholar.google.com/citations?hl=en&user=WvufSLAAAAAJ">Ishan Misra</a>,
    <a href="https://scholar.google.com/citations?hl=en&user=h8u3ll8AAAAJ">Nicolas Carion</a>,
    <br>
    <em>ICCV 2021</em>, &nbsp <font color="red"><strong>(Oral Presentation, top 3% of submissions)</strong></font>
    <br>
    <a href="https://ashkamath.github.io/mdetr_page">project page</a>
    /
    <a href="https://arxiv.org/pdf/2104.12763">arXiv</a>
    <p></p>
    <p>We step away from existing approaches to multi-modal understanding that involve frozen pre-trained object
        detectors trained on a fixed label set, and instead achieve true end-to-end multi-modal understanding by
        detecting objects that are referred to in free form text. We make it possible to detect and reason over novel
        combination of object classes and attributes like "a pink elephant".</p>
</td>
</tbody></table>

<!-- <h2>Here's a list of my publications-</h2>
<strong><a href="https://arxiv.org/pdf/2104.12763.pdf">MDETR - Modulated Detection for End-to-End Multi-Modal Understanding</a> </strong><br>
      Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, Nicolas Carion. International Conference on Computer Vision (ICCV) 2021. Selected for oral presentation (top 3% of submitted papers)
	<br><br>
<strong><a href="https://arxiv.org/pdf/2005.00247.pdf">AdapterFusion: Non-Destructive Task Composition for Transfer Learning</a> </strong><br>
      Jonas Pfeiffer , Aishwarya Kamath , Andreas Ruckle , Kyunghyun Cho , Iryna Gurevych.  Selected for oral presentation at European Chapter of the ACL (EACL) 2021. 
	<br><br>
<strong><a href="https://openreview.net/pdf?id=HylaEWcTT7">A Survey on Semantic Parsing</a> </strong><br>
      Aishwarya Kamath and Rajarshi Das, AKBC 2019.
      <br><br> 
<strong><a href="https://arxiv.org/pdf/1909.04547.pdf">What do Deep Networks Like to Read?</a> </strong><br>
      Jonas Pfeiffer* , Aishwarya Kamath* , Sebastian Ruder. Arxiv preprint, September 2019. 
	<br><br>
<strong><a href="https://www.aclweb.org/anthology/W19-4310">Specializing Distributional Vectors of All Words for Lexical Entailment</a> </strong><br>
      Aishwarya Kamath* , Jonas Pfeiffer* , Edoardo M. Ponti, Goran Glava≈°, Ivan Vulic. <b> Best paper award </b> at Representation Learning for NLP Workshop at ACL 2019.
      <br><br> 
<strong><a href="http://aclweb.org/anthology/N18-2021">Training Structured Prediction Energy Networks with Indirect Supervision</a> </strong><br>
      Amirmohammad Rooshenas, Aishwarya Kamath, and Andrew McCallum, NAACL-HLT 2018.
      <br><br>    -->






	


